{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple Chain with Retrieval\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a simple RAG chain with ChatOllama, HuggingFaceEmbeddings and Chroma. \n",
    "\n",
    "Process: \n",
    "\n",
    "1. Retrieve documents from chroma db based on query\n",
    "2. Invoke chain with retrieved documents as input\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load llm model via ollama\n",
    "- load embedding model via ollama with `ollama pull pull bge-m3` (if not yet done)\n",
    "- create chroma db client\n",
    "- create prompt template for summarization\n",
    "- create simple chain with following steps: retrieved documents, prompt, model, output parser\n",
    "- create query and perform similarity search with a query\n",
    "- invoke chain and pass retrieved documents to the chain\n",
    "\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)\n",
    "- [Streaming in Langchain](https://python.langchain.com/docs/concepts/streaming/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "model = ChatOllama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = OllamaEmbeddings(model=\"bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "{\"detail\":\"Not Found\"} (trace ID: 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\base_http_client.py:100\u001b[39m, in \u001b[36mBaseHTTPClient._raise_chroma_error\u001b[39m\u001b[34m(resp)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'http://localhost:8000/api/v2/auth/identity'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\client.py:122\u001b[39m, in \u001b[36mClient.get_user_identity\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_user_identity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity < granularity:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\fastapi.py:209\u001b[39m, in \u001b[36mFastAPI.get_user_identity\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFastAPI.get_user_identity\u001b[39m\u001b[33m\"\u001b[39m, OpenTelemetryGranularity.OPERATION)\n\u001b[32m    207\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_user_identity\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> UserIdentity:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserIdentity(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/auth/identity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\fastapi.py:115\u001b[39m, in \u001b[36mFastAPI._make_request\u001b[39m\u001b[34m(self, method, path, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m response = \u001b[38;5;28mself\u001b[39m._session.request(method, url, **cast(Any, kwargs))\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mBaseHTTPClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_raise_chroma_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m orjson.loads(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\base_http_client.py:104\u001b[39m, in \u001b[36mBaseHTTPClient._raise_chroma_error\u001b[39m\u001b[34m(resp)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trace_id:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (trace ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m(resp.text))\n",
      "\u001b[31mException\u001b[39m: {\"detail\":\"Not Found\"} (trace ID: 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m client = \u001b[43mchromadb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHttpClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocalhost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallow_reset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manonymized_telemetry\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEFAULT_TENANT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEFAULT_DATABASE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create a collection\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ADD HERE YOUR CODE\u001b[39;00m\n\u001b[32m     18\u001b[39m collection_name = \u001b[33m\"\u001b[39m\u001b[33mai_model_book\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\__init__.py:293\u001b[39m, in \u001b[36mHttpClient\u001b[39m\u001b[34m(host, port, ssl, headers, settings, tenant, database)\u001b[39m\n\u001b[32m    290\u001b[39m settings.chroma_server_ssl_enabled = ssl\n\u001b[32m    291\u001b[39m settings.chroma_server_headers = headers\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClientCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\client.py:75\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, tenant, database, settings)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Get the root system component we want to interact with\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m._server = \u001b[38;5;28mself\u001b[39m._system.instance(ServerAPI)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m user_identity = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_user_identity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m maybe_tenant, maybe_database = maybe_set_tenant_and_database(\n\u001b[32m     78\u001b[39m     user_identity,\n\u001b[32m     79\u001b[39m     overwrite_singleton_tenant_database_access_from_auth=settings.chroma_overwrite_singleton_tenant_database_access_from_auth,\n\u001b[32m     80\u001b[39m     user_provided_tenant=tenant,\n\u001b[32m     81\u001b[39m     user_provided_database=database,\n\u001b[32m     82\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# this should not happen unless types are invalidated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Leina\\OneDrive\\Uni\\3 Semester\\Technische Informatik\\Labor\\DH-build-your-own-chatbot\\.venv\\Lib\\site-packages\\chromadb\\api\\client.py:131\u001b[39m, in \u001b[36mClient.get_user_identity\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[31mValueError\u001b[39m: {\"detail\":\"Not Found\"} (trace ID: 0)"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# Create a collection\n",
    "# ADD HERE YOUR CODE\n",
    "collection_name = \"ai_model_book\"\n",
    "collection = client.get_collection(name=collection_name)\n",
    "\n",
    "# Create chromadb\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client = Chroma(client=client,\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "docs = vector_db_from_client.similarity_search(search_query, k=3)\n",
    "\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"docs\": formatted_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple stream the chain output\n",
    "for chunk in chain.stream({\"docs\": formatted_docs}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex async event streaming\n",
    "async for event in chain.astream_events({\"docs\": formatted_docs}, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Q&A with RAG\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Implement a Q/A retrieval chain with ChatOllama, HuggingFaceEmbeddings and Chroma\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create RAG-Q/A prompt template\n",
    "- create retriever from vector db client (instead of manually passing in docs, we automatically retrieve them from our vector store based on the user question)\n",
    "- create simple chain with following steps: retriever, formatting retrieved docs, user question, prompt, model, output parser\n",
    "- create question for Q/A retrieval chain\n",
    "- invoke chain and with question\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [RAG with Ollama](https://python.langchain.com/v0.2/docs/tutorials/local_rag/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "rag_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "retriever = vector_db_from_client.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is supervised learning?\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "qa_rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex async event streaming\n",
    "async for event in qa_rag_chain.astream_events(question, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative: Using pre-built ConversationalRetrievalChain Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db_from_client.as_retriever()\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    model, retriever=retriever, memory=memory, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex async event streaming\n",
    "async for event in qa_chain.astream_events(\"What is supervised learning?\", version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex async event streaming\n",
    "async for event in qa_chain.astream_events(\"Which algorithms can be used there?\", version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
